{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "KetsguPpS2GO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mii2DOhzSx_I",
        "outputId": "de801e1e-a243-453e-c115-a652aff24cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run these cells once; comment out after installation)\n",
        "!pip install torch_geometric --quiet\n",
        "!pip install scikit-learn pandas scipy scikit-network --quiet\n",
        "!pip install scikit-network --quiet\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.semi_supervised import LabelPropagation as SKLabelPropagation\n",
        "\n",
        "# scikit-network\n",
        "from sknetwork.classification import Propagation as SKNPropagation\n",
        "\n",
        "# PyTorch Geometric\n",
        "from torch_geometric.nn import LabelPropagation as PyGLabelPropagation\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# !pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu > /dev/null\n",
        "# !pip install  dgl==1.1.0 -f https://data.dgl.ai/wheels/repo.html > /dev/null\n",
        "# # !pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n",
        "# os.environ['TORCH'] = torch.__version__\n",
        "# os.environ['DGLBACKEND'] = \"pytorch\"\n",
        "# import dgl\n",
        "# import dgl.function as fn\n",
        "\n",
        "# For dataset loading via torch_geometric\n",
        "from torch_geometric.datasets import Flickr, AmazonProducts, Yelp, Taobao\n",
        "\n",
        "# Set device: use GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "yEXw1xuNewbv",
        "outputId": "180da802-a16a-4e12-b8db-167ad601f700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-QF8IkMgdkR",
        "outputId": "542adb59-c6cf-40dd-950d-2fd5065be52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Management"
      ],
      "metadata": {
        "id": "dCctAkeOTEGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_nodes_dynamic(feature_dim, dense=False, safety_factor=0.8):\n",
        "    \"\"\"\n",
        "    Estimate the maximum number of nodes that can be processed.\n",
        "    For a sparse graph, each node costs ~ feature_dim*4 bytes.\n",
        "    For a dense graph (all pairwise edges), memory ~ N^2 * 4 bytes.\n",
        "    \"\"\"\n",
        "    if device.type == 'cuda':\n",
        "        free_mem, _ = torch.cuda.mem_get_info()  # in bytes\n",
        "        free_mem = int(free_mem * safety_factor)\n",
        "    else:\n",
        "        free_mem = int(psutil.virtual_memory().available * safety_factor)\n",
        "    if dense:\n",
        "        # For a dense similarity matrix (float32), need ~ N^2 * 4 bytes.\n",
        "        max_nodes = int((free_mem / 4) ** 0.5)\n",
        "    else:\n",
        "        bytes_per_node = feature_dim * 4\n",
        "        max_nodes = free_mem // bytes_per_node\n",
        "    return max_nodes\n",
        "\n",
        "STATIC_LIMITS = {\n",
        "    'Flickr': 100000,\n",
        "    'AmazonProducts': 50000,\n",
        "    'Yelp': 100000,\n",
        "    'Taobao': 50000\n",
        "}\n",
        "\n",
        "def get_max_nodes_static(dataset_name):\n",
        "    \"\"\"Return preset maximum nodes for a given dataset.\"\"\"\n",
        "    return STATIC_LIMITS.get(dataset_name, None)\n"
      ],
      "metadata": {
        "id": "WekzKO7MS75j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VLP"
      ],
      "metadata": {
        "id": "0yFfih23TM8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_node_vectors(V):\n",
        "    \"\"\"\n",
        "    Transform node vectors so that for each node vector v_i,\n",
        "    the inner product equals normalized cosine similarity.\n",
        "    Achieved by: v_i_new = [v_i/||v_i||, 1] / sqrt(2)\n",
        "    \"\"\"\n",
        "    norms = torch.norm(V, p=2, dim=1, keepdim=True) + 1e-10\n",
        "    V_norm = V / norms\n",
        "    ones_col = torch.ones(V_norm.size(0), 1, device=V_norm.device)\n",
        "    V_aug = torch.cat((V_norm, ones_col), dim=1)\n",
        "    V_transformed = V_aug / torch.sqrt(torch.tensor(2.0, device=V_aug.device))\n",
        "    return V_transformed\n",
        "\n",
        "def compute_degrees(V):\n",
        "    \"\"\"\n",
        "    Given transformed node vectors V (N x (d+1)), compute degrees:\n",
        "      deg_i = (v_i^T (sum_j v_j)) - (v_i^T v_i)\n",
        "    \"\"\"\n",
        "    row_sums = torch.sum(V, dim=0)\n",
        "    degrees = torch.matmul(V, row_sums) - torch.sum(V * V, dim=1)\n",
        "    return degrees\n",
        "\n",
        "def lp_one_iter(V, Y, degrees):\n",
        "    \"\"\"\n",
        "    One iteration of VLP.\n",
        "    V: transformed node vectors (N x (d+1))\n",
        "    Y: label probability matrix (N x K)\n",
        "    degrees: precomputed degree vector (N,)\n",
        "\n",
        "    Returns:\n",
        "      Y_next = (1/deg_i) * (V (V^T Y) - (v_i^T v_i)*Y)\n",
        "    \"\"\"\n",
        "    self_loop = torch.sum(V * V, dim=1)  # shape: (N,)\n",
        "    A_Y = torch.matmul(V, torch.matmul(V.T, Y))\n",
        "    Y_update = A_Y - self_loop.unsqueeze(1) * Y\n",
        "    inv_deg = 1.0 / (degrees + 1e-10)\n",
        "    Y_next = inv_deg.unsqueeze(1) * Y_update\n",
        "    return Y_next\n",
        "\n",
        "def lp(V, Y_init, num_iter):\n",
        "    \"\"\"\n",
        "    Run VLP for num_iter iterations.\n",
        "    \"\"\"\n",
        "    degrees = compute_degrees(V)\n",
        "    Y = Y_init\n",
        "    for t in range(num_iter):\n",
        "        Y = lp_one_iter(V, Y, degrees)\n",
        "    return Y\n",
        "\n",
        "def vlp_run(features, num_iter, num_labels):\n",
        "    \"\"\"\n",
        "    Run our VLP approach for 'num_iter' label propagation steps.\n",
        "    \"\"\"\n",
        "    n_nodes = features.size(0)\n",
        "    # Transform node vectors\n",
        "    V = transform_node_vectors(features)\n",
        "    deg = compute_degrees(V)\n",
        "    # Assign random labels (one-hot)\n",
        "    Y_init = assign_initial_labels(n_nodes, num_labels)\n",
        "\n",
        "    start_t = time.time()\n",
        "    Y = Y_init.clone()\n",
        "    for _ in range(num_iter):\n",
        "        Y = lp_one_iter(V, Y, deg)\n",
        "    elapsed = time.time() - start_t\n",
        "\n",
        "    return elapsed"
      ],
      "metadata": {
        "id": "Qwx_UapwTGUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other methods"
      ],
      "metadata": {
        "id": "sLN_8JTAqBBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def can_build_dense_graph(num_nodes):\n",
        "    \"\"\"\n",
        "    Estimate if building an n x n adjacency matrix (float32) is feasible.\n",
        "    We'll do a rough check: n^2 * 4 bytes, plus overhead. If it exceeds\n",
        "    80% of available memory, we say it's not feasible.\n",
        "    \"\"\"\n",
        "    free_mem = psutil.virtual_memory().available\n",
        "    needed = num_nodes**2 * 4\n",
        "    # We'll require that needed < (0.8 * free_mem)\n",
        "    return needed < 0.8 * free_mem"
      ],
      "metadata": {
        "id": "Hgn4wJwOrf5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_kernel(X, Y):\n",
        "    X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-10)\n",
        "    Y_norm = Y / (np.linalg.norm(Y, axis=1, keepdims=True) + 1e-10)\n",
        "    sim = np.dot(X_norm, Y_norm.T)\n",
        "    return (sim + 1.0) / 2.0"
      ],
      "metadata": {
        "id": "PQ5rpNH_tjV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# scikit-learn Label Propagation using a Custom Kernel\n",
        "def sklearn_lp_run(features, num_iter):\n",
        "    \"\"\"\n",
        "    Run scikit-learn's LabelPropagation using a custom kernel function.\n",
        "    If the full dense matrix is infeasible, return -1.\n",
        "    \"\"\"\n",
        "    n_nodes = features.size(0)\n",
        "    if not can_build_dense_graph(n_nodes):\n",
        "        return -1\n",
        "\n",
        "    # Use the original features on CPU\n",
        "    feats_cpu = features.detach().cpu().numpy().astype(np.float32)\n",
        "    # Use our custom kernel to compute the dense similarity matrix internally.\n",
        "    # LabelPropagation will call my_kernel(feats_cpu, feats_cpu)\n",
        "    from sklearn.semi_supervised import LabelPropagation\n",
        "    start_t = time.time()\n",
        "    lp_model = SKLabelPropagation(kernel=my_kernel, max_iter=num_iter)\n",
        "    # Create label array: label first 100, rest -1.\n",
        "    # labels = -np.ones(n_nodes, dtype=int)\n",
        "    labels = np.random.randint(0, 50, size=n_nodes)\n",
        "    lp_model.fit(feats_cpu, labels)\n",
        "    elapsed = time.time() - start_t\n",
        "    return elapsed\n",
        "\n",
        "###############################################################################\n",
        "# scikit-network Label Propagation\n",
        "def skn_lp_run(features, num_iter):\n",
        "    \"\"\"\n",
        "    Run scikit-network's Propagation on a dense adjacency matrix.\n",
        "    If the full dense graph is infeasible, return -1.\n",
        "    \"\"\"\n",
        "    n_nodes = features.size(0)\n",
        "    if not can_build_dense_graph(n_nodes):\n",
        "        return -1\n",
        "\n",
        "    feats_cpu = features.detach().cpu().numpy().astype(np.float32)\n",
        "    norms = np.linalg.norm(feats_cpu, axis=1, keepdims=True) + 1e-10\n",
        "    feats_norm = feats_cpu / norms\n",
        "    sim = feats_norm @ feats_norm.T\n",
        "    A = (sim + 1.0) / 2.0\n",
        "    np.fill_diagonal(A, 0.0)\n",
        "    # Build a sparse adjacency matrix from A\n",
        "    row_idx, col_idx = np.nonzero(A)\n",
        "    vals = A[row_idx, col_idx]\n",
        "    adjacency = sp.csr_matrix((vals, (row_idx, col_idx)), shape=(n_nodes, n_nodes))\n",
        "\n",
        "    from sknetwork.classification import Propagation\n",
        "    label_dict = {i: int(np.random.randint(0, 50)) for i in range(n_nodes)}\n",
        "    start_t = time.time()\n",
        "    prop = SKNPropagation(n_iter=num_iter, weighted=True)\n",
        "    prop.fit_predict(adjacency, labels=label_dict)\n",
        "    elapsed = time.time() - start_t\n",
        "    del adjacency, A, sim\n",
        "    gc.collect()\n",
        "    return elapsed\n",
        "\n",
        "###############################################################################\n",
        "# PyTorch Geometric Label Propagation\n",
        "def pyg_lp_run(features, num_iter):\n",
        "    \"\"\"\n",
        "    Run PyTorch Geometric's LabelPropagation on a dense graph.\n",
        "    If constructing a full dense graph is infeasible, return -1.\n",
        "    \"\"\"\n",
        "    n_nodes = features.size(0)\n",
        "    if n_nodes > 11000:\n",
        "      return -1\n",
        "\n",
        "    # Compute the full dense similarity matrix on CPU\n",
        "    feats_cpu = features.detach().cpu().numpy().astype(np.float32)\n",
        "    norms = np.linalg.norm(feats_cpu, axis=1, keepdims=True) + 1e-10\n",
        "    feats_norm = feats_cpu / norms\n",
        "    sim = feats_norm @ feats_norm.T\n",
        "    sim = (sim + 1.0) / 2.0\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "    row_idx, col_idx = np.nonzero(sim)\n",
        "    edge_src = torch.tensor(row_idx, device=device, dtype=torch.long)\n",
        "    edge_dst = torch.tensor(col_idx, device=device, dtype=torch.long)\n",
        "    edge_index = torch.stack([edge_src, edge_dst], dim=0)\n",
        "\n",
        "    # Fix: Instead of negative labels, assign a random valid label (0 to 4) for every node.\n",
        "    label_arr = torch.randint(0, 50, (n_nodes,), device=device, dtype=torch.long)\n",
        "\n",
        "    from torch_geometric.nn import LabelPropagation\n",
        "    start_t = time.time()\n",
        "    lp = LabelPropagation(num_layers=num_iter, alpha=0.8)\n",
        "    lp(label_arr, edge_index)\n",
        "    elapsed = time.time() - start_t\n",
        "\n",
        "    del sim, row_idx, col_idx, edge_src, edge_dst, edge_index\n",
        "    gc.collect()\n",
        "    return elapsed\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# DGL Label Propagation\n",
        "def dgl_lp_run(features, num_iter=5):\n",
        "    \"\"\"\n",
        "    Run label propagation using DGL on the full dense graph.\n",
        "    The function computes the full dense similarity matrix on CPU,\n",
        "    converts it to a SciPy CSR matrix, then builds a DGL graph via dgl.from_scipy().\n",
        "    We remove any existing self–loops and then re–add self–loops as per recommended DGL practice.\n",
        "    Finally, we try to move the graph to GPU. If that fails, we remain on CPU.\n",
        "    If the number of nodes exceeds 50,000 or building the dense graph is infeasible, returns -1.\n",
        "    \"\"\"\n",
        "    n_nodes = features.size(0)\n",
        "    if n_nodes > 50000 or not can_build_dense_graph(n_nodes):\n",
        "        return -1\n",
        "\n",
        "    # Compute full dense similarity on CPU using normalized cosine similarity\n",
        "    feats_cpu = features.detach().cpu().numpy().astype(np.float32)\n",
        "    norms = np.linalg.norm(feats_cpu, axis=1, keepdims=True) + 1e-10\n",
        "    feats_norm = feats_cpu / norms\n",
        "    sim = feats_norm @ feats_norm.T\n",
        "    sim = (sim + 1.0) / 2.0\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "\n",
        "    # Convert the dense similarity matrix to a sparse CSR matrix\n",
        "    row_idx, col_idx = np.nonzero(sim)\n",
        "    vals = sim[row_idx, col_idx]\n",
        "    adjacency = sp.csr_matrix((vals, (row_idx, col_idx)), shape=(n_nodes, n_nodes))\n",
        "\n",
        "    # Build a DGL graph from the sparse adjacency\n",
        "    g = dgl.from_scipy(adjacency)\n",
        "\n",
        "    # Remove any preexisting self–loops and then add self–loops\n",
        "    g = dgl.remove_self_loop(g)\n",
        "    g = dgl.add_self_loop(g)\n",
        "\n",
        "    # Attempt to move the graph to GPU; if it fails, fallback to CPU.\n",
        "    try:\n",
        "        g = g.to(device)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to move DGL graph to GPU, using CPU instead:\", e)\n",
        "        g = g.to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Initialize node labels (one-hot) on the same device as g.\n",
        "    Y_init = assign_initial_labels(n_nodes, 5).to(g.device)\n",
        "    g.ndata['h'] = Y_init.clone()\n",
        "\n",
        "    start_t = time.time()\n",
        "    for _ in range(num_iter):\n",
        "        # Use the standard update_all pattern with a temporary field.\n",
        "        g.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_new'))\n",
        "        g.ndata['h'] = g.ndata.pop('h_new')\n",
        "    elapsed = time.time() - start_t\n",
        "\n",
        "    del sim, row_idx, col_idx, adjacency\n",
        "    gc.collect()\n",
        "    return elapsed\n"
      ],
      "metadata": {
        "id": "Ph7xpAAGp_c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Generation"
      ],
      "metadata": {
        "id": "k_iFBDDjUBOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_similarity_graph(features, top_k=None):\n",
        "    \"\"\"\n",
        "    Construct a graph from node features.\n",
        "    If top_k is provided, compute a k-NN graph (sparse);\n",
        "    if top_k is None, compute the full dense graph (all pairwise edges).\n",
        "\n",
        "    Returns:\n",
        "      edge_index_src, edge_index_dst as torch tensors on device.\n",
        "    \"\"\"\n",
        "    N, dim = features.shape\n",
        "    features_norm = torch.nn.functional.normalize(features, p=2, dim=1)\n",
        "    if top_k is None:\n",
        "        # Full dense graph: compute dense similarity matrix, set diagonal to 0,\n",
        "        # and then extract indices for all nonzero entries.\n",
        "        with torch.no_grad():\n",
        "            S = torch.matmul(features_norm, features_norm.T)\n",
        "        S = (S + 1) / 2  # normalize between 0 and 1\n",
        "        S.fill_diagonal_(0)\n",
        "        edge_indices = torch.nonzero(S, as_tuple=False)\n",
        "        edge_index_src = edge_indices[:, 0]\n",
        "        edge_index_dst = edge_indices[:, 1]\n",
        "        return edge_index_src.to(device), edge_index_dst.to(device)\n",
        "    else:\n",
        "        # Sparse k-NN graph (as before)\n",
        "        all_src = []\n",
        "        all_dst = []\n",
        "        if device.type == 'cuda':\n",
        "            free_mem = torch.cuda.mem_get_info()[0]\n",
        "        else:\n",
        "            free_mem = psutil.virtual_memory().available\n",
        "        target_bytes = int(free_mem * 0.25)\n",
        "        max_M = max(1, target_bytes // (N * 4))\n",
        "        batch_size = min(N, max_M)\n",
        "        if batch_size < N:\n",
        "            print(f\"Using batch size {batch_size} for similarity computation to fit memory.\")\n",
        "        for i in range(0, N, batch_size):\n",
        "            end = min(i + batch_size, N)\n",
        "            batch_features = features_norm[i:end]\n",
        "            sim_matrix = batch_features @ features_norm.T\n",
        "            k = min(top_k + 1, N)\n",
        "            sims, idxs = torch.topk(sim_matrix, k=k, dim=1)\n",
        "            idxs = idxs.cpu().numpy()\n",
        "            for j, node_idx in enumerate(range(i, end)):\n",
        "                neigh_indices = [nid for nid in idxs[j] if nid != node_idx][:top_k]\n",
        "                all_src.extend([node_idx] * len(neigh_indices))\n",
        "                all_dst.extend(neigh_indices)\n",
        "        edge_index_src = torch.tensor(all_src, device=device, dtype=torch.long)\n",
        "        edge_index_dst = torch.tensor(all_dst, device=device, dtype=torch.long)\n",
        "        return edge_index_src, edge_index_dst\n"
      ],
      "metadata": {
        "id": "JjD3p1JpT6Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "iaHUd3NrUFD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_flickr_dataset():\n",
        "    # Example using Torch Geometric's Flickr dataset (dummy version)\n",
        "    from torch_geometric.datasets import Flickr\n",
        "    data = Flickr(root='data/Flickr')\n",
        "    return data.data.x  # Node features\n",
        "\n",
        "def load_amazon_dataset():\n",
        "    from torch_geometric.datasets import AmazonProducts\n",
        "    data = AmazonProducts(root='data/AmazonProducts')\n",
        "    return data.data.x\n",
        "\n",
        "def load_yelp_dataset():\n",
        "    from torch_geometric.datasets import Yelp\n",
        "    data = Yelp(root='data/Yelp')\n",
        "    return data.data.x\n",
        "\n",
        "def load_taobao_dataset():\n",
        "    # Load Taobao dataset using Torch Geometric\n",
        "    from torch_geometric.datasets import Taobao\n",
        "    data = Taobao(root='data/Taobao')\n",
        "    # Get edge indices for user-item and item-category\n",
        "    u_t_i = data.data['user', 'to', 'item']['edge_index']\n",
        "    i_t_c = data.data['item', 'to', 'category']['edge_index']\n",
        "    # Join relationships: create a user-category linkage via items.\n",
        "    def join_relationships_count(a_to_i, i_to_c):\n",
        "        a_ids = a_to_i[0].tolist()\n",
        "        inter_ids = a_to_i[1].tolist()\n",
        "        inter_from_c = i_to_c[0].tolist()\n",
        "        c_ids = i_to_c[1].tolist()\n",
        "        mapping = {}\n",
        "        for inter, cid in zip(inter_from_c, c_ids):\n",
        "            if inter not in mapping:\n",
        "                mapping[inter] = []\n",
        "            mapping[inter].append(cid)\n",
        "        joined = []\n",
        "        for a, inter in zip(a_ids, inter_ids):\n",
        "            if inter in mapping:\n",
        "                for cid in mapping[inter]:\n",
        "                    joined.append((a, cid))\n",
        "        df = pd.DataFrame(joined, columns=['user', 'category'])\n",
        "        count_df = df.groupby(['user', 'category']).size().reset_index(name='count')\n",
        "        return count_df\n",
        "\n",
        "    taobao_df = join_relationships_count(u_t_i, i_t_c)\n",
        "    def filter_by_threshold(df, threshold_percentage=15, col=1):\n",
        "        col_counts = df.iloc[:, col].value_counts()\n",
        "        max_count = col_counts.max()\n",
        "        threshold = max_count * (threshold_percentage / 100.0)\n",
        "        filtered = df[df.iloc[:, col].isin(col_counts[col_counts >= threshold].index)]\n",
        "        return filtered\n",
        "    filtered_df = filter_by_threshold(taobao_df, threshold_percentage=15)\n",
        "    def df_to_matrix(df):\n",
        "        c1, c2, c3 = df.columns.tolist()\n",
        "        pivot_df = df.pivot(index=c1, columns=c2, values=c3)\n",
        "        pivot_df.fillna(0, inplace=True)\n",
        "        return pivot_df\n",
        "    def convert_to_tfidf(pivot_df):\n",
        "        from sklearn.feature_extraction.text import TfidfTransformer\n",
        "        tfidf_transformer = TfidfTransformer()\n",
        "        tfidf_matrix = tfidf_transformer.fit_transform(pivot_df)\n",
        "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=pivot_df.index, columns=pivot_df.columns)\n",
        "        return tfidf_df\n",
        "    pivot_df = df_to_matrix(filtered_df)\n",
        "    tfidf_df = convert_to_tfidf(pivot_df)\n",
        "    # Convert to torch tensor\n",
        "    tfidf_matrix = torch.tensor(tfidf_df.values, dtype=torch.float32, device=device)\n",
        "    return tfidf_matrix"
      ],
      "metadata": {
        "id": "R7pXIWjtUDAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Label Assignment"
      ],
      "metadata": {
        "id": "YolgOtJ-VbOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_initial_labels(num_nodes, num_labels, seed=42):\n",
        "    \"\"\"\n",
        "    Randomly assign each node one of num_labels; return a one-hot matrix (num_nodes, num_labels).\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    rand_labels = np.random.randint(0, num_labels, size=num_nodes)\n",
        "    Y_init = np.zeros((num_nodes, num_labels), dtype=np.float32)\n",
        "    for i, lbl in enumerate(rand_labels):\n",
        "        Y_init[i, lbl] = 1.0\n",
        "    return torch.tensor(Y_init, device=device)\n"
      ],
      "metadata": {
        "id": "dmDAG3XIUGx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment function"
      ],
      "metadata": {
        "id": "ng6oB2lrXQXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lp_experiment(dataset_name, thresh_list, num_labels=5, num_iter=1, n_runs=5):\n",
        "    results = []\n",
        "\n",
        "    # Load dataset features based on dataset_name\n",
        "    if dataset_name == \"Flickr\":\n",
        "        full_features = load_flickr_dataset()\n",
        "    elif dataset_name == \"AmazonProducts\":\n",
        "        full_features = load_amazon_dataset()\n",
        "    elif dataset_name == \"Yelp\":\n",
        "        full_features = load_yelp_dataset()\n",
        "    elif dataset_name == \"Taobao\":\n",
        "        full_features = load_taobao_dataset()\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset name.\")\n",
        "\n",
        "    full_features = full_features.to(device)\n",
        "    total_nodes = full_features.size(0)\n",
        "    feat_dim = full_features.size(1)\n",
        "    print(f\"{dataset_name}: total nodes = {total_nodes}, feature dim = {feat_dim}\")\n",
        "\n",
        "    num_nodes_cap = total_nodes  # use all nodes as cap\n",
        "\n",
        "    for thresh in thresh_list:\n",
        "        num_nodes = int(num_nodes_cap * thresh)\n",
        "        if num_nodes < 1:\n",
        "            continue\n",
        "        print(f\"\\nProcessing {dataset_name} with threshold {thresh} ({num_nodes} nodes)\")\n",
        "        sub_features = full_features[:num_nodes]\n",
        "\n",
        "        # Method 1: VLP (vector-based LP, always runs)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        vlp_times = []\n",
        "        for _ in range(n_runs):\n",
        "            start_time = time.time()\n",
        "            _ = vlp_run(sub_features, num_iter=num_iter, num_labels=num_labels)\n",
        "            vlp_times.append(time.time() - start_time)\n",
        "        time_vlp = np.mean(vlp_times)\n",
        "\n",
        "        # For the other methods, if num_nodes exceeds 50,000, record -1\n",
        "        non_vlp_time = {}\n",
        "        if num_nodes > 20000:\n",
        "            non_vlp_time['sklearn'] = -1\n",
        "            non_vlp_time['skn'] = -1\n",
        "            non_vlp_time['pyg'] = -1\n",
        "            # non_vlp_time['dgl'] = -1\n",
        "        else:\n",
        "            sklearn_times = []\n",
        "            skn_times = []\n",
        "            pyg_times = []\n",
        "            dgl_times = []\n",
        "            for _ in range(n_runs):\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                start_time = time.time()\n",
        "                _ = sklearn_lp_run(sub_features, num_iter=num_iter)\n",
        "                sklearn_times.append(time.time() - start_time)\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                start_time = time.time()\n",
        "                _ = skn_lp_run(sub_features, num_iter=num_iter)\n",
        "                skn_times.append(time.time() - start_time)\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                start_time = time.time()\n",
        "                _ = pyg_lp_run(sub_features, num_iter=num_iter)\n",
        "                pyg_times.append(time.time() - start_time)\n",
        "\n",
        "                # torch.cuda.empty_cache()\n",
        "                # gc.collect()\n",
        "                # start_time = time.time()\n",
        "                # _ = dgl_lp_run(sub_features, num_iter=num_iter)\n",
        "                # dgl_times.append(time.time() - start_time)\n",
        "\n",
        "            non_vlp_time['sklearn'] = np.mean(sklearn_times)\n",
        "            non_vlp_time['skn'] = np.mean(skn_times)\n",
        "            non_vlp_time['pyg'] = np.mean(pyg_times)\n",
        "            # non_vlp_time['dgl'] = np.mean(dgl_times)\n",
        "\n",
        "        results.append({\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Threshold\": thresh,\n",
        "            \"NodesUsed\": num_nodes,\n",
        "            \"Time_VLP\": round(time_vlp, 4),\n",
        "            \"Time_sklearn\": round(non_vlp_time['sklearn'], 4) if non_vlp_time['sklearn'] > 0 else non_vlp_time['sklearn'],\n",
        "            \"Time_skn\": round(non_vlp_time['skn'], 4) if non_vlp_time['skn'] > 0 else non_vlp_time['skn'],\n",
        "            \"Time_pyg\": round(non_vlp_time['pyg'], 4) if non_vlp_time['pyg'] > 0 else non_vlp_time['pyg'],\n",
        "            # \"Time_dgl\": round(non_vlp_time['dgl'], 4) if non_vlp_time['dgl'] > 0 else non_vlp_time['dgl']\n",
        "        })\n",
        "\n",
        "        del sub_features\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(f\"{dataset_name}_lp_experiment_results.csv\", index=False)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "mtvtY5UnVdhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Experiments for All Datasets"
      ],
      "metadata": {
        "id": "psoogFngV38J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define threshold fractions (as fraction of the maximum nodes available)\n",
        "# threshold_list = [0.05, 0.1, 0.15, 0.2]\n",
        "num_labels = 50   # Number of initial labels/classes\n",
        "num_iter = 100     # Number of LP iterations (set to 1; adjust if needed)"
      ],
      "metadata": {
        "id": "ZLN5SQncV5CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running experiments on Flickr\")\n",
        "results_flickr = run_lp_experiment(\"Flickr\", [0.05, 0.1, .15, .2, 0.5, 1], num_labels, num_iter)\n",
        "print(results_flickr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAlrC5jCXT_s",
        "outputId": "2bd6a98c-4395-4be5-ed6c-9738cb1916fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiments on Flickr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flickr: total nodes = 89250, feature dim = 500\n",
            "\n",
            "Processing Flickr with threshold 0.05 (4462 nodes)\n",
            "\n",
            "Processing Flickr with threshold 0.1 (8925 nodes)\n",
            "\n",
            "Processing Flickr with threshold 0.15 (13387 nodes)\n",
            "\n",
            "Processing Flickr with threshold 0.2 (17850 nodes)\n",
            "\n",
            "Processing Flickr with threshold 0.5 (44625 nodes)\n",
            "\n",
            "Processing Flickr with threshold 1 (89250 nodes)\n",
            "  Dataset  Threshold  NodesUsed  Time_VLP  Time_sklearn  Time_skn  Time_pyg\n",
            "0  Flickr       0.05       4462    0.1273        0.2012    1.2401    0.7766\n",
            "1  Flickr       0.10       8925    0.0199        0.6553    4.2275    7.6131\n",
            "2  Flickr       0.15      13387    0.0279        1.3853    9.2052    0.0000\n",
            "3  Flickr       0.20      17850    0.0387        2.6493   17.1691    0.0000\n",
            "4  Flickr       0.50      44625    0.0819       -1.0000   -1.0000   -1.0000\n",
            "5  Flickr       1.00      89250    0.1377       -1.0000   -1.0000   -1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running experiments on AmazonProducts\")\n",
        "results_amazon = run_lp_experiment(\"AmazonProducts\", [0.002, 0.004, 0.006, 0.008, 0.01,.5,1], num_labels, num_iter)\n",
        "print(results_amazon)"
      ],
      "metadata": {
        "id": "JMXSqWreX56u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5008eb61-6c0e-46fa-ae70-9b3805d5641a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiments on AmazonProducts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://drive.usercontent.google.com/download?id=17qhNA8H1IpbkkR-T2BmPQm8QNW5do-aa&confirm=t\n",
            "Downloading https://drive.usercontent.google.com/download?id=10SW8lCvAj-kb6ckkfTOC5y0l8XXdtMxj&confirm=t\n",
            "Downloading https://drive.usercontent.google.com/download?id=1LIl4kimLfftj4-7NmValuWyCQE8AaE7P&confirm=t\n",
            "Downloading https://drive.usercontent.google.com/download?id=1npK9xlmbnjNkV80hK2Q68wTEVOFjnt4K&confirm=t\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AmazonProducts: total nodes = 1569960, feature dim = 200\n",
            "\n",
            "Processing AmazonProducts with threshold 0.002 (3139 nodes)\n",
            "\n",
            "Processing AmazonProducts with threshold 0.004 (6279 nodes)\n",
            "\n",
            "Processing AmazonProducts with threshold 0.006 (9419 nodes)\n",
            "\n",
            "Processing AmazonProducts with threshold 0.008 (12559 nodes)\n",
            "\n",
            "Processing AmazonProducts with threshold 0.01 (15699 nodes)\n",
            "\n",
            "Processing AmazonProducts with threshold 0.5 (784980 nodes)\n",
            "\n",
            "Processing AmazonProducts with threshold 1 (1569960 nodes)\n",
            "          Dataset  Threshold  NodesUsed  Time_VLP  Time_sklearn  Time_skn  \\\n",
            "0  AmazonProducts      0.002       3139    0.0181        0.0749    0.7098   \n",
            "1  AmazonProducts      0.004       6279    0.0191        0.2580    2.2355   \n",
            "2  AmazonProducts      0.006       9419    0.0219        0.6032    5.0631   \n",
            "3  AmazonProducts      0.008      12559    0.0226        1.1324    9.2570   \n",
            "4  AmazonProducts      0.010      15699    0.0254        1.8379   13.4113   \n",
            "5  AmazonProducts      0.500     784980    0.6848       -1.0000   -1.0000   \n",
            "6  AmazonProducts      1.000    1569960    1.3373       -1.0000   -1.0000   \n",
            "\n",
            "   Time_pyg  \n",
            "0    0.4813  \n",
            "1    2.1096  \n",
            "2   11.2609  \n",
            "3    0.0000  \n",
            "4    0.0000  \n",
            "5   -1.0000  \n",
            "6   -1.0000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running experiments on Yelp\")\n",
        "results_yelp = run_lp_experiment(\"Yelp\", [0.004, 0.008, 0.012, 0.016, 0.02, .5, 1], num_labels, num_iter)\n",
        "print(results_yelp)"
      ],
      "metadata": {
        "id": "h8IAHuKhX8SZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdfd93aa-7a31-41f3-c2a8-49a054944765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiments on Yelp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yelp: total nodes = 716847, feature dim = 300\n",
            "\n",
            "Processing Yelp with threshold 0.004 (2867 nodes)\n",
            "\n",
            "Processing Yelp with threshold 0.008 (5734 nodes)\n",
            "\n",
            "Processing Yelp with threshold 0.012 (8602 nodes)\n",
            "\n",
            "Processing Yelp with threshold 0.016 (11469 nodes)\n",
            "\n",
            "Processing Yelp with threshold 0.02 (14336 nodes)\n",
            "\n",
            "Processing Yelp with threshold 0.5 (358423 nodes)\n",
            "\n",
            "Processing Yelp with threshold 1 (716847 nodes)\n",
            "  Dataset  Threshold  NodesUsed  Time_VLP  Time_sklearn  Time_skn  Time_pyg\n",
            "0    Yelp      0.004       2867    0.0590        0.0797    0.6447    0.4634\n",
            "1    Yelp      0.008       5734    0.0172        0.2879    1.8705    1.7989\n",
            "2    Yelp      0.012       8602    0.0190        0.5437    3.8604    7.0309\n",
            "3    Yelp      0.016      11469    0.0195        0.9487    6.7375    0.0000\n",
            "4    Yelp      0.020      14336    0.0252        1.4439   11.4248    0.0000\n",
            "5    Yelp      0.500     358423    0.3960       -1.0000   -1.0000   -1.0000\n",
            "6    Yelp      1.000     716847    0.7716       -1.0000   -1.0000   -1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running experiments on Taobao\")\n",
        "results_taobao = run_lp_experiment(\"Taobao\", [0.005, 0.01, 0.015, 0.02, 0.025, 0.03,.5,1], num_labels, num_iter)\n",
        "print(results_taobao)"
      ],
      "metadata": {
        "id": "cccpPH7GX9oS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec03043d-ad6c-4387-c369-dc0e8ede04b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiments on Taobao\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://alicloud-dev.oss-cn-hangzhou.aliyuncs.com/UserBehavior.csv.zip\n",
            "Extracting data/Taobao/raw/UserBehavior.csv.zip\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taobao: total nodes = 936946, feature dim = 66\n",
            "\n",
            "Processing Taobao with threshold 0.005 (4684 nodes)\n",
            "\n",
            "Processing Taobao with threshold 0.01 (9369 nodes)\n",
            "\n",
            "Processing Taobao with threshold 0.015 (14054 nodes)\n",
            "\n",
            "Processing Taobao with threshold 0.02 (18738 nodes)\n",
            "\n",
            "Processing Taobao with threshold 0.025 (23423 nodes)\n",
            "\n",
            "Processing Taobao with threshold 0.03 (28108 nodes)\n",
            "\n",
            "Processing Taobao with threshold 0.5 (468473 nodes)\n",
            "\n",
            "Processing Taobao with threshold 1 (936946 nodes)\n",
            "  Dataset  Threshold  NodesUsed  Time_VLP  Time_sklearn  Time_skn  Time_pyg\n",
            "0  Taobao      0.005       4684    0.0173        0.1692    1.2994    1.2526\n",
            "1  Taobao      0.010       9369    0.0190        0.5787    4.6596   10.8839\n",
            "2  Taobao      0.015      14054    0.0206        1.2624    9.9202    0.0000\n",
            "3  Taobao      0.020      18738    0.0233        2.2510   17.4895    0.0000\n",
            "4  Taobao      0.025      23423    0.0221       -1.0000   -1.0000   -1.0000\n",
            "5  Taobao      0.030      28108    0.0235       -1.0000   -1.0000   -1.0000\n",
            "6  Taobao      0.500     468473    0.3064       -1.0000   -1.0000   -1.0000\n",
            "7  Taobao      1.000     936946    0.5941       -1.0000   -1.0000   -1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = pd.concat([results_flickr, results_amazon, results_yelp, results_taobao], ignore_index=True)\n",
        "all_results.to_csv(\"all_lp_experiment_results.csv\", index=False)\n",
        "print(\"\\n*** Final LP Experiment Results ***\")\n",
        "display(all_results)"
      ],
      "metadata": {
        "id": "aJSoLZruX-45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3hCtQ-GYhQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}